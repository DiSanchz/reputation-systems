{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58794d33",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0872b",
   "metadata": {},
   "source": [
    "Inference on the simulation parameters is carried out employing Sequential Neural Posterior Estimation (SNPE), implemented by the `sbi` (acronym for simulation based inference) package. This notebooks explores the code behind the process of building a posterior distribution and get samples from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea001b6",
   "metadata": {},
   "source": [
    "### A. Imports and preliminary steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669835e",
   "metadata": {},
   "source": [
    "First of all, the relevant imports are carried out in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff30a5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext autoreload\\n%autoreload 2\\n\\n# Just a formatting related plugin\\n%load_ext nb_black\\n\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\n\\nimport sys\\n\\nsys.path.append(\\\"../\\\")\\n\\nimport multiprocessing as mp\\n\\nfrom collections import deque\\nfrom pathlib import Path\\nfrom typing import Dict, Optional\\n\\nimport arviz\\nimport pickle\\n\\nimport numpy as np\\nimport pandas as pd\\nimport pyreadr\\nimport sbi\\nimport sbi.utils as sbi_utils\\nimport seaborn as sns\\nimport statsmodels.formula.api as smf\\nimport torch\\n\\nfrom joblib import Parallel, delayed\\nfrom matplotlib.lines import Line2D\\nfrom scipy.stats import ttest_ind\\nfrom snpe.inference import inference_class\\nfrom snpe.simulations import simulator_class, marketplace_simulator_class\\nfrom snpe.embeddings.embeddings_to_ratings import EmbeddingRatingPredictor\\nfrom snpe.utils.statistics import review_histogram_correlation\\nfrom snpe.utils.tqdm_utils import tqdm_joblib\\nfrom tqdm import tqdm\\n\\n# Set plotting parameters\\nsns.set(style=\\\"white\\\", context=\\\"talk\\\", font_scale=2.5)\\nsns.set_color_codes(palette=\\\"colorblind\\\")\\nsns.set_style(\\\"ticks\\\", {\\\"axes.linewidth\\\": 2.0})\";\n",
       "                var nbb_formatted_code = \"%load_ext autoreload\\n%autoreload 2\\n\\n# Just a formatting related plugin\\n%load_ext nb_black\\n\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\n\\nimport sys\\n\\nsys.path.append(\\\"../\\\")\\n\\nimport multiprocessing as mp\\n\\nfrom collections import deque\\nfrom pathlib import Path\\nfrom typing import Dict, Optional\\n\\nimport arviz\\nimport pickle\\n\\nimport numpy as np\\nimport pandas as pd\\nimport pyreadr\\nimport sbi\\nimport sbi.utils as sbi_utils\\nimport seaborn as sns\\nimport statsmodels.formula.api as smf\\nimport torch\\n\\nfrom joblib import Parallel, delayed\\nfrom matplotlib.lines import Line2D\\nfrom scipy.stats import ttest_ind\\nfrom snpe.inference import inference_class\\nfrom snpe.simulations import simulator_class, marketplace_simulator_class\\nfrom snpe.embeddings.embeddings_to_ratings import EmbeddingRatingPredictor\\nfrom snpe.utils.statistics import review_histogram_correlation\\nfrom snpe.utils.tqdm_utils import tqdm_joblib\\nfrom tqdm import tqdm\\n\\n# Set plotting parameters\\nsns.set(style=\\\"white\\\", context=\\\"talk\\\", font_scale=2.5)\\nsns.set_color_codes(palette=\\\"colorblind\\\")\\nsns.set_style(\\\"ticks\\\", {\\\"axes.linewidth\\\": 2.0})\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Just a formatting related plugin\n",
    "%load_ext nb_black\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import arviz\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import sbi\n",
    "import sbi.utils as sbi_utils\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import torch\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import ttest_ind\n",
    "from snpe.inference import inference_class\n",
    "from snpe.simulations import simulator_class, marketplace_simulator_class\n",
    "from snpe.embeddings.embeddings_to_ratings import EmbeddingRatingPredictor\n",
    "from snpe.utils.statistics import review_histogram_correlation\n",
    "from snpe.utils.tqdm_utils import tqdm_joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set plotting parameters\n",
    "sns.set(style=\"white\", context=\"talk\", font_scale=2.5)\n",
    "sns.set_color_codes(palette=\"colorblind\")\n",
    "sns.set_style(\"ticks\", {\"axes.linewidth\": 2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c81c8d",
   "metadata": {},
   "source": [
    "Then the path where the inference output is going to be stored, as well as where the relevant inputs for the functions are located is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6365b672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"ARTIFACT_PATH = Path(\\\"../../../kill\\\")\";\n",
       "                var nbb_formatted_code = \"ARTIFACT_PATH = Path(\\\"../../../kill\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ARTIFACT_PATH = Path(\"../../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cefe6",
   "metadata": {},
   "source": [
    "### B. Building a posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1627882",
   "metadata": {},
   "source": [
    "The function in the code cell below is designed to carry out inference on the basis of time series simulation output.\n",
    "\n",
    "The first lines of the function are devoted to initialize two tensors containing the upper and lower bounds for the parameter distributions, set the device for the training of the inference model and finally instantiate the inferrer, which in this case is an object of the `TimeSeriesInference` class (See `inference_class.py`).\n",
    "\n",
    "Once this is done, the `load_simulator` method of the inferrer is called. This method loads a simulator of the appropriate type (as specified when invoking the function) which will subsequently load a simulation output to be fed to the inference model. Then the inferrer's method `infer_snpe_posterior` is called, this method will:\n",
    "\n",
    "   1. Create an embedding net employing a subset of the simulations.\n",
    "   2. Employ the embedding net to obtain a function that will build a density estimator for learning the posterior (posterior net)\n",
    "   3. Conduct SNPE  with the posterior net and the prior bounds provided.\n",
    "   4. Builds a posterior from the neural density estimator.\n",
    "    \n",
    "Finally the method `save_inference` stores posterior in a `.pkl` file alongside other relevant pieces of information about the simulation and the inference procedure.\n",
    "\n",
    "The arguments to provide to the function are the following:\n",
    "\n",
    "- `device`: Device where the inference excercise will take place. Must be a string, either 'cuda' or 'cpu'.\n",
    "- `simulator_type`: Type of the simulator to be loaded by the inferrer (e.g. double_herding, marketplace...).\n",
    "- `simulation_type`: Modality of simulation that will be loaded. Must be str 'timeseries' or 'histogram'.\n",
    "- `params`: Dictionary of parameters to provide to the embedding net creator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7365ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"def infer_and_save_posterior(\\n    device: str, simulator_type: str, simulation_type: str, params: Dict\\n) -> None:\\n    parameter_prior = sbi_utils.BoxUniform(\\n        low=torch.tensor([0.0, 0.0, 0.0, 0.0, 0.5, 0.25, 0.25, 0.5]).type(\\n            torch.FloatTensor\\n        ),\\n        high=torch.tensor([4.0, 4.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0]).type(\\n            torch.FloatTensor\\n        ),\\n        device=device,\\n    )\\n    inferrer = inference_class.TimeSeriesInference(\\n        parameter_prior=parameter_prior, device=device\\n    )\\n    inferrer.load_simulator(\\n        dirname=ARTIFACT_PATH,\\n        simulator_type=simulator_type,\\n        simulation_type=simulation_type,\\n    )\\n    batch_size = params.pop(\\\"batch_size\\\")\\n    learning_rate = params.pop(\\\"learning_rate\\\")\\n    hidden_features = params.pop(\\\"hidden_features\\\")\\n    num_transforms = params.pop(\\\"num_transforms\\\")\\n    inferrer.infer_snpe_posterior(\\n        embedding_net_conf=params,\\n        batch_size=batch_size,\\n        learning_rate=learning_rate,\\n        hidden_features=hidden_features,\\n        num_transforms=num_transforms,\\n    )\\n    inferrer.save_inference(ARTIFACT_PATH)\";\n",
       "                var nbb_formatted_code = \"def infer_and_save_posterior(\\n    device: str, simulator_type: str, simulation_type: str, params: Dict\\n) -> None:\\n    parameter_prior = sbi_utils.BoxUniform(\\n        low=torch.tensor([0.0, 0.0, 0.0, 0.0, 0.5, 0.25, 0.25, 0.5]).type(\\n            torch.FloatTensor\\n        ),\\n        high=torch.tensor([4.0, 4.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0]).type(\\n            torch.FloatTensor\\n        ),\\n        device=device,\\n    )\\n    inferrer = inference_class.TimeSeriesInference(\\n        parameter_prior=parameter_prior, device=device\\n    )\\n    inferrer.load_simulator(\\n        dirname=ARTIFACT_PATH,\\n        simulator_type=simulator_type,\\n        simulation_type=simulation_type,\\n    )\\n    batch_size = params.pop(\\\"batch_size\\\")\\n    learning_rate = params.pop(\\\"learning_rate\\\")\\n    hidden_features = params.pop(\\\"hidden_features\\\")\\n    num_transforms = params.pop(\\\"num_transforms\\\")\\n    inferrer.infer_snpe_posterior(\\n        embedding_net_conf=params,\\n        batch_size=batch_size,\\n        learning_rate=learning_rate,\\n        hidden_features=hidden_features,\\n        num_transforms=num_transforms,\\n    )\\n    inferrer.save_inference(ARTIFACT_PATH)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def infer_and_save_posterior(\n",
    "    device: str, simulator_type: str, simulation_type: str, params: Dict\n",
    ") -> None:\n",
    "    parameter_prior = sbi_utils.BoxUniform(\n",
    "        low=torch.tensor([0.0, 0.0, 0.0, 0.0, 0.5, 0.25, 0.25, 0.5]).type(\n",
    "            torch.FloatTensor\n",
    "        ),\n",
    "        high=torch.tensor([4.0, 4.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0]).type(\n",
    "            torch.FloatTensor\n",
    "        ),\n",
    "        device=device,\n",
    "    )\n",
    "    inferrer = inference_class.TimeSeriesInference(\n",
    "        parameter_prior=parameter_prior, device=device\n",
    "    )\n",
    "    inferrer.load_simulator(\n",
    "        dirname=ARTIFACT_PATH,\n",
    "        simulator_type=simulator_type,\n",
    "        simulation_type=simulation_type,\n",
    "    )\n",
    "    batch_size = params.pop(\"batch_size\")\n",
    "    learning_rate = params.pop(\"learning_rate\")\n",
    "    hidden_features = params.pop(\"hidden_features\")\n",
    "    num_transforms = params.pop(\"num_transforms\")\n",
    "    inferrer.infer_snpe_posterior(\n",
    "        embedding_net_conf=params,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        hidden_features=hidden_features,\n",
    "        num_transforms=num_transforms,\n",
    "    )\n",
    "    inferrer.save_inference(ARTIFACT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88521c7b",
   "metadata": {},
   "source": [
    "The following cell defines the dictionary of hyperparameters for the inference model that has to be provided as argument to `infer_and_save_posterior` to shape the embedding net. This particular set of values have been obtained by carrying out a hyperparameter optimization procedure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d02c9e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# diccionario de par\\u00e1metros requerido para utilizar la funci\\u00f3n\\n\\ninference_params = {\\n    \\\"batch_size\\\": 64,\\n    \\\"learning_rate\\\": 3.1e-4,\\n    \\\"hidden_features\\\": 70,\\n    \\\"num_transforms\\\": 8,\\n    \\\"num_conv_layers\\\": 2,\\n    \\\"num_channels\\\": 9,\\n    \\\"conv_kernel_size\\\": 17,\\n    \\\"maxpool_kernel_size\\\": 11,\\n    \\\"num_dense_layers\\\": 2,\\n}\";\n",
       "                var nbb_formatted_code = \"# diccionario de par\\u00e1metros requerido para utilizar la funci\\u00f3n\\n\\ninference_params = {\\n    \\\"batch_size\\\": 64,\\n    \\\"learning_rate\\\": 3.1e-4,\\n    \\\"hidden_features\\\": 70,\\n    \\\"num_transforms\\\": 8,\\n    \\\"num_conv_layers\\\": 2,\\n    \\\"num_channels\\\": 9,\\n    \\\"conv_kernel_size\\\": 17,\\n    \\\"maxpool_kernel_size\\\": 11,\\n    \\\"num_dense_layers\\\": 2,\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# diccionario de parámetros requerido para utilizar la función\n",
    "\n",
    "inference_params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 3.1e-4,\n",
    "    \"hidden_features\": 70,\n",
    "    \"num_transforms\": 8,\n",
    "    \"num_conv_layers\": 2,\n",
    "    \"num_channels\": 9,\n",
    "    \"conv_kernel_size\": 17,\n",
    "    \"maxpool_kernel_size\": 11,\n",
    "    \"num_dense_layers\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939950d",
   "metadata": {},
   "source": [
    "Now, with all the ingredients in place, the `infer_and_save_posterior` function can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3589e2ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sbi/utils/torchutils.py:28: UserWarning: GPU was selected as a device for training the neural network. Note that we expect **no** significant speed ups in training for the default architectures we provide. Using the GPU will be effective only for large neural networks with operations that are fast on the GPU, e.g., for a CNN or RNN `embedding_net`.\n",
      "  \"GPU was selected as a device for training the neural network. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding net created: \n",
      " Sequential(\n",
      "  (0): Conv1d(5, 9, kernel_size=(17,), stride=(1,), padding=(8,))\n",
      "  (1): LeakyReLU(negative_slope=0.01)\n",
      "  (2): Conv1d(9, 9, kernel_size=(17,), stride=(1,), padding=(16,), dilation=(2,))\n",
      "  (3): MaxPool1d(kernel_size=11, stride=11, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Flatten(start_dim=1, end_dim=-1)\n",
      "  (5): LeakyReLU(negative_slope=0.01)\n",
      "  (6): Linear(in_features=459, out_features=64, bias=True)\n",
      "  (7): LeakyReLU(negative_slope=0.01)\n",
      "  (8): Linear(in_features=64, out_features=32, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sbi/utils/user_input_checks.py:697: UserWarning: Parameters theta has device 'cpu'. Moving theta to the data_device 'cuda:0'.Training will proceed on device 'cuda:0'.\n",
      "  f\"Parameters theta has device '{theta.device}'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 127 epochs.\n",
      "        -------------------------\n",
      "        ||||| ROUND 1 STATS |||||:\n",
      "        -------------------------\n",
      "        Epochs trained: 127\n",
      "        Best validation performance: 3.0781\n",
      "        -------------------------\n",
      "        \n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"infer_and_save_posterior(\\\"cuda\\\", \\\"marketplace\\\", \\\"timeseries\\\", inference_params)\";\n",
       "                var nbb_formatted_code = \"infer_and_save_posterior(\\\"cuda\\\", \\\"marketplace\\\", \\\"timeseries\\\", inference_params)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "infer_and_save_posterior(\"cuda\", \"marketplace\", \"timeseries\", inference_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a43f567",
   "metadata": {},
   "source": [
    "### C. Sampling from the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ce1eb",
   "metadata": {},
   "source": [
    "The following function `sample_posterior_with_observed` gets parameter samples form the posterior distribution built by `infer_and_save_posterior` conditioned on the observations provided.\n",
    "\n",
    "- `device`: Device where the sampling operation will take place. Must a string, either 'cuda' or 'cpu'.\n",
    "- `simulator_type`: Type of the simulator to be loaded by the inferrer instance in charge of drawing the samples (e.g. double_herding, marketplace...).\n",
    "- `simulation_type`: Modality of simulation that will be loaded. Must be str 'timeseries' or 'histogram'.\n",
    "- `num_samples`: Number of samples to take from the defined posterior.\n",
    "- `observations`: Simulation data for which the inferrer will samples. \n",
    "\n",
    "The function returns an numpy array of the shape (num_samples, < number_of_products_in_observations >, < number_of_parameters >) with the sampled posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c02bd2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"def sample_posterior_with_observed(\\n    device: str,\\n    observations: np.array,\\n    num_samples: int,\\n    simulator_type: str,\\n    simulation_type: str,\\n) -> np.ndarray:\\n    # The parameter prior doesn't matter here as it will be overridden by that of the loaded inference object\\n    parameter_prior = sbi.utils.BoxUniform(\\n        low=torch.tensor([0.0, 0.0, 0.0, 0.0, 0.5, 0.25, 0.25, 0.5]).type(\\n            torch.FloatTensor\\n        ),\\n        high=torch.tensor([4.0, 4.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0]).type(\\n            torch.FloatTensor\\n        ),\\n        device=device,\\n    )\\n    inferrer = inference_class.TimeSeriesInference(\\n        parameter_prior=parameter_prior, device=device\\n    )\\n    inferrer.load_simulator(\\n        dirname=ARTIFACT_PATH,\\n        simulator_type=simulator_type,\\n        simulation_type=simulation_type,\\n    )\\n    inferrer.load_inference(dirname=ARTIFACT_PATH)\\n    posterior_samples = inferrer.get_posterior_samples(\\n        observations, num_samples=num_samples\\n    )\\n    return posterior_samples\";\n",
       "                var nbb_formatted_code = \"def sample_posterior_with_observed(\\n    device: str,\\n    observations: np.array,\\n    num_samples: int,\\n    simulator_type: str,\\n    simulation_type: str,\\n) -> np.ndarray:\\n    # The parameter prior doesn't matter here as it will be overridden by that of the loaded inference object\\n    parameter_prior = sbi.utils.BoxUniform(\\n        low=torch.tensor([0.0, 0.0, 0.0, 0.0, 0.5, 0.25, 0.25, 0.5]).type(\\n            torch.FloatTensor\\n        ),\\n        high=torch.tensor([4.0, 4.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0]).type(\\n            torch.FloatTensor\\n        ),\\n        device=device,\\n    )\\n    inferrer = inference_class.TimeSeriesInference(\\n        parameter_prior=parameter_prior, device=device\\n    )\\n    inferrer.load_simulator(\\n        dirname=ARTIFACT_PATH,\\n        simulator_type=simulator_type,\\n        simulation_type=simulation_type,\\n    )\\n    inferrer.load_inference(dirname=ARTIFACT_PATH)\\n    posterior_samples = inferrer.get_posterior_samples(\\n        observations, num_samples=num_samples\\n    )\\n    return posterior_samples\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sample_posterior_with_observed(\n",
    "    device: str,\n",
    "    observations: np.array,\n",
    "    num_samples: int,\n",
    "    simulator_type: str,\n",
    "    simulation_type: str,\n",
    ") -> np.ndarray:\n",
    "    # The parameter prior doesn't matter here as it will be overridden by that of the loaded inference object\n",
    "    parameter_prior = sbi.utils.BoxUniform(\n",
    "        low=torch.tensor([0.0, 0.0, 0.0, 0.0, 0.5, 0.25, 0.25, 0.5]).type(\n",
    "            torch.FloatTensor\n",
    "        ),\n",
    "        high=torch.tensor([4.0, 4.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0]).type(\n",
    "            torch.FloatTensor\n",
    "        ),\n",
    "        device=device,\n",
    "    )\n",
    "    inferrer = inference_class.TimeSeriesInference(\n",
    "        parameter_prior=parameter_prior, device=device\n",
    "    )\n",
    "    inferrer.load_simulator(\n",
    "        dirname=ARTIFACT_PATH,\n",
    "        simulator_type=simulator_type,\n",
    "        simulation_type=simulation_type,\n",
    "    )\n",
    "    inferrer.load_inference(dirname=ARTIFACT_PATH)\n",
    "    posterior_samples = inferrer.get_posterior_samples(\n",
    "        observations, num_samples=num_samples\n",
    "    )\n",
    "    return posterior_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc2b83",
   "metadata": {},
   "source": [
    "In the example below, `sample_posterior_with_observed` is set to get 10 parameter samples conditioned on a marketplace timeseries simulation (`timeseries_data`) which is imported immediately before actually calling the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86eb0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_data = np.load(ARTIFACT_PATH / 'timeseries_data.npy')\n",
    "\n",
    "posterior_samples = sample_posterior_with_observed(\"cuda\", timeseries_data, 10, \"marketplace\", \"timeseries\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
